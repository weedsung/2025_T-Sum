"""
Week 07: í…ìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬ ê¸°ì´ˆ - ìì—°ì–´ì²˜ë¦¬ & ê°ì„±ë¶„ì„
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
import re
from collections import Counter
from wordcloud import WordCloud
import warnings
warnings.filterwarnings('ignore')

print("=== í…ìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬ ê¸°ì´ˆ ===\n")

# 1. ìì—°ì–´ì²˜ë¦¬ ì†Œê°œ
print("1. ìì—°ì–´ì²˜ë¦¬(NLP)ë€?")
print("-" * 25)
print("ğŸ“ ìì—°ì–´ì²˜ë¦¬: ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ê¸°ìˆ ")
print("ğŸ” ì£¼ìš” ì‘ìš©:")
print("  - ê°ì„± ë¶„ì„ (ë¦¬ë·°, ì†Œì…œë¯¸ë””ì–´)")
print("  - ê¸°ê³„ ë²ˆì—­ (êµ¬ê¸€ ë²ˆì—­)")
print("  - ì±—ë´‡ ë° ê°€ìƒ ë¹„ì„œ")
print("  - ë¬¸ì„œ ë¶„ë¥˜ ë° ìš”ì•½")
print("  - ìŠ¤íŒ¸ ë©”ì¼ í•„í„°ë§")
print()

# 2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ê¸°ì´ˆ
print("2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ê¸°ì´ˆ")
print("-" * 20)

# ì˜ˆì‹œ ë¬¸ì„œë“¤
documents = [
    "ì´ ì˜í™”ëŠ” ì •ë§ ì¬ë¯¸ìˆì—ˆìŠµë‹ˆë‹¤! ê°•ë ¥ ì¶”ì²œí•©ë‹ˆë‹¤.",
    "ë„ˆë¬´ ì§€ë£¨í•œ ì˜í™”ì˜€ì–´ìš”. ì‹œê°„ ì•„ê¹Œì› ìŠµë‹ˆë‹¤.",
    "ë°°ìš°ë“¤ì˜ ì—°ê¸°ê°€ í›Œë¥­í–ˆê³  ìŠ¤í† ë¦¬ë„ ê°ë™ì ì´ì—ˆìŠµë‹ˆë‹¤.",
    "ë³„ë¡œì˜€ì–´ìš”. ëˆì´ ì•„ê¹Œìš´ ì˜í™”ì…ë‹ˆë‹¤.",
    "ìµœê³ ì˜ ì˜í™”! ë‹¤ì‹œ ë³´ê³  ì‹¶ì–´ìš”."
]

print("ğŸ“„ ì›ë³¸ í…ìŠ¤íŠ¸:")
for i, doc in enumerate(documents, 1):
    print(f"{i}. {doc}")
print()

# ê¸°ë³¸ ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤
def clean_text(text):
    """ê¸°ë³¸ í…ìŠ¤íŠ¸ ì •ì œ"""
    # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ì–´, ìˆ«ì, ê³µë°±ë§Œ ë‚¨ê¹€)
    text = re.sub(r'[^ê°€-í£ã„±-ã…ã…-ã…£a-zA-Z0-9\s]', '', text)
    # ì—°ì† ê³µë°±ì„ í•˜ë‚˜ë¡œ
    text = re.sub(r'\s+', ' ', text)
    # ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()
    return text

def simple_tokenize(text):
    """ê°„ë‹¨í•œ í† í°í™” (ê³µë°± ê¸°ì¤€)"""
    return text.split()

print("ğŸ§¹ ì „ì²˜ë¦¬ í›„:")
cleaned_docs = [clean_text(doc) for doc in documents]
for i, doc in enumerate(cleaned_docs, 1):
    print(f"{i}. {doc}")
    print(f"   í† í°: {simple_tokenize(doc)}")
print()

# 3. ê°ì„± ë¶„ì„ìš© ë°ì´í„° ì¤€ë¹„
print("3. ê°ì„± ë¶„ì„ìš© ë°ì´í„° ì¤€ë¹„")
print("-" * 25)

# ì˜í™” ë¦¬ë·° ì˜ˆì‹œ ë°ì´í„° (ë” ë§ì€ ë°ì´í„°)
movie_reviews = [
    # ê¸ì • ë¦¬ë·°
    "ì´ ì˜í™”ëŠ” ì •ë§ í›Œë¥­í–ˆìŠµë‹ˆë‹¤ ê°ë™ì ì´ê³  ì¬ë¯¸ìˆì—ˆì–´ìš”",
    "ìµœê³ ì˜ ì˜í™”ì…ë‹ˆë‹¤ ë°°ìš°ë“¤ì˜ ì—°ê¸°ê°€ ë›°ì–´ë‚¬ì–´ìš”",
    "ê°•ë ¥ ì¶”ì²œí•©ë‹ˆë‹¤ ìŠ¤í† ë¦¬ê°€ ë§¤ìš° í¥ë¯¸ë¡œì› ìŠµë‹ˆë‹¤",
    "ì •ë§ ì¬ë¯¸ìˆê²Œ ë´¤ìŠµë‹ˆë‹¤ ë‹¤ì‹œ ë³´ê³  ì‹¶ì–´ìš”",
    "ê°ë™ì ì¸ ì˜í™”ì˜€ì–´ìš” ëˆˆë¬¼ì´ ë‚¬ìŠµë‹ˆë‹¤",
    "í›Œë¥­í•œ ì‘í’ˆì´ë„¤ìš” ì‹œê°„ê°€ëŠ” ì¤„ ëª°ëì–´ìš”",
    "ìµœê³ ì˜ˆìš” ì •ë§ ì¢‹ì•˜ìŠµë‹ˆë‹¤",
    "ë©‹ì§„ ì˜í™”ì˜€ìŠµë‹ˆë‹¤ ì¶”ì²œí•´ìš”",
    "ì •ë§ ì¦ê±°ì› ì–´ìš” ì¢‹ì€ ì˜í™”ì…ë‹ˆë‹¤",
    "ê°ë™ë°›ì•˜ìŠµë‹ˆë‹¤ í›Œë¥­í•œ ìŠ¤í† ë¦¬ì˜ˆìš”",
    
    # ë¶€ì • ë¦¬ë·°  
    "ë„ˆë¬´ ì§€ë£¨í•œ ì˜í™”ì˜€ì–´ìš” ì‹œê°„ ì•„ê¹Œì› ìŠµë‹ˆë‹¤",
    "ë³„ë¡œì˜€ìŠµë‹ˆë‹¤ ëˆì´ ì•„ê¹Œì›Œìš”",
    "ì¬ë¯¸ì—†ì–´ìš” ëê¹Œì§€ ë³´ê¸° í˜ë“¤ì—ˆìŠµë‹ˆë‹¤",
    "ì‹¤ë§ìŠ¤ëŸ¬ìš´ ì˜í™”ì˜€ìŠµë‹ˆë‹¤ ê¸°ëŒ€ê°€ ì»¸ëŠ”ë°",
    "ìŠ¤í† ë¦¬ê°€ ì—‰ì„±í–ˆì–´ìš” ì•„ì‰¬ìš´ ì‘í’ˆì…ë‹ˆë‹¤",
    "ì§€ë£¨í•˜ê³  ì¬ë¯¸ì—†ì—ˆì–´ìš” ì¶”ì²œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤",
    "ìµœì•…ì˜ ì˜í™”ì…ë‹ˆë‹¤ ì‹œê°„ë‚­ë¹„ì˜€ì–´ìš”",
    "ë„ˆë¬´ ì•„ì‰¬ìš´ ì‘í’ˆì´ì—ìš” ê¸°ëŒ€ ì´í•˜ì˜€ìŠµë‹ˆë‹¤",
    "ë³„ë¡œì˜ˆìš” ëˆì•„ê¹Œìš´ ì˜í™”ì…ë‹ˆë‹¤",
    "ì¬ë¯¸ì—†ê³  ì§€ë£¨í–ˆì–´ìš” ì‹¤ë§í–ˆìŠµë‹ˆë‹¤"
]

# ë¼ë²¨ ìƒì„± (1: ê¸ì •, 0: ë¶€ì •)
labels = [1] * 10 + [0] * 10  # ê¸ì • 10ê°œ, ë¶€ì • 10ê°œ

# DataFrame ìƒì„±
df = pd.DataFrame({
    'review': movie_reviews,
    'sentiment': labels
})

print(f"ğŸ“Š ë°ì´í„° í¬ê¸°: {df.shape}")
print(f"ê°ì„±ë³„ ë¶„í¬:")
print(df['sentiment'].value_counts())
print("\në°ì´í„° ì˜ˆì‹œ:")
print(df.head())
print()

# 4. ë‹¨ì–´ ë¹ˆë„ ë¶„ì„
print("4. ë‹¨ì–´ ë¹ˆë„ ë¶„ì„")
print("-" * 15)

# ëª¨ë“  í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
all_text = ' '.join(df['review'])
words = simple_tokenize(all_text)

# ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
word_freq = Counter(words)
most_common = word_freq.most_common(10)

print("ğŸ”¤ ê°€ì¥ ìì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤:")
for word, freq in most_common:
    print(f"  {word}: {freq}ë²ˆ")
print()

# ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± (í•œê¸€ í°íŠ¸ ë¬¸ì œë¡œ ê°„ë‹¨íˆ ì‹œê°í™”)
plt.figure(figsize=(12, 5))

# ë‹¨ì–´ ë¹ˆë„ ë§‰ëŒ€ê·¸ë˜í”„
plt.subplot(1, 2, 1)
words_list, freqs = zip(*most_common)
plt.barh(words_list, freqs)
plt.title('ë‹¨ì–´ ë¹ˆë„')
plt.xlabel('ë¹ˆë„')

# ê°ì„±ë³„ ë‹¨ì–´ ê¸¸ì´ ë¶„í¬
plt.subplot(1, 2, 2)
df['word_count'] = df['review'].apply(lambda x: len(simple_tokenize(x)))

for sentiment in [0, 1]:
    sentiment_data = df[df['sentiment'] == sentiment]['word_count']
    label = 'ë¶€ì •' if sentiment == 0 else 'ê¸ì •'
    plt.hist(sentiment_data, alpha=0.7, label=label, bins=10)

plt.xlabel('ë‹¨ì–´ ê°œìˆ˜')
plt.ylabel('ë¹ˆë„')
plt.title('ê°ì„±ë³„ ë¦¬ë·° ê¸¸ì´ ë¶„í¬')
plt.legend()

plt.tight_layout()
plt.show()

# 5. í…ìŠ¤íŠ¸ ë²¡í„°í™”
print("5. í…ìŠ¤íŠ¸ ë²¡í„°í™”")
print("-" * 15)

# ë°ì´í„° ë¶„í• 
X = df['review']
y = df['sentiment']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print(f"í›ˆë ¨ ë°ì´í„°: {len(X_train)}ê°œ")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test)}ê°œ")

# 1) Bag of Words (BoW)
print("\nğŸ“Š Bag of Words ë²¡í„°í™”:")
bow_vectorizer = CountVectorizer(max_features=50, stop_words=None)
X_train_bow = bow_vectorizer.fit_transform(X_train)
X_test_bow = bow_vectorizer.transform(X_test)

print(f"BoW íŠ¹ì„± ìˆ˜: {X_train_bow.shape[1]}")
print(f"íŠ¹ì„± ì˜ˆì‹œ: {list(bow_vectorizer.get_feature_names_out())[:10]}")

# 2) TF-IDF
print("\nğŸ“ˆ TF-IDF ë²¡í„°í™”:")
tfidf_vectorizer = TfidfVectorizer(max_features=50, stop_words=None)
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

print(f"TF-IDF íŠ¹ì„± ìˆ˜: {X_train_tfidf.shape[1]}")
print()

# 6. ê°ì„± ë¶„ì„ ëª¨ë¸ í•™ìŠµ
print("6. ê°ì„± ë¶„ì„ ëª¨ë¸ í•™ìŠµ")
print("-" * 20)

# ëª¨ë¸ë“¤ ì •ì˜
models = {
    'Naive Bayes (BoW)': (MultinomialNB(), X_train_bow, X_test_bow),
    'Naive Bayes (TF-IDF)': (MultinomialNB(), X_train_tfidf, X_test_tfidf),
    'Logistic Regression (BoW)': (LogisticRegression(random_state=42), X_train_bow, X_test_bow),
    'Logistic Regression (TF-IDF)': (LogisticRegression(random_state=42), X_train_tfidf, X_test_tfidf)
}

results = {}

print("ğŸ¤– ëª¨ë¸ í•™ìŠµ ë° í‰ê°€:")
for name, (model, X_tr, X_te) in models.items():
    # ëª¨ë¸ í•™ìŠµ
    model.fit(X_tr, y_train)
    
    # ì˜ˆì¸¡
    y_pred = model.predict(X_te)
    
    # ì •í™•ë„ ê³„ì‚°
    accuracy = (y_pred == y_test).mean()
    results[name] = accuracy
    
    print(f"ğŸ“Š {name}: {accuracy:.3f}")

print()

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ
best_model_name = max(results, key=results.get)
best_model, best_X_train, best_X_test = models[best_model_name]

print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}")
print(f"ì •í™•ë„: {results[best_model_name]:.3f}")

# ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸
y_pred_best = best_model.predict(best_X_test)
print("\nğŸ“‹ ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
print(classification_report(y_test, y_pred_best, target_names=['ë¶€ì •', 'ê¸ì •']))

# í˜¼ë™ í–‰ë ¬
cm = confusion_matrix(y_test, y_pred_best)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['ë¶€ì •', 'ê¸ì •'], yticklabels=['ë¶€ì •', 'ê¸ì •'])
plt.title(f'{best_model_name}\ní˜¼ë™ í–‰ë ¬')
plt.xlabel('ì˜ˆì¸¡')
plt.ylabel('ì‹¤ì œ')
plt.show()

print()

# 7. ìƒˆë¡œìš´ ë¦¬ë·° ê°ì„± ì˜ˆì¸¡
print("7. ìƒˆë¡œìš´ ë¦¬ë·° ê°ì„± ì˜ˆì¸¡")
print("-" * 20)

# ìƒˆë¡œìš´ ë¦¬ë·°ë“¤
new_reviews = [
    "ì •ë§ ìµœê³ ì˜ ì˜í™”ì˜€ìŠµë‹ˆë‹¤ ê°ë™ë°›ì•˜ì–´ìš”",
    "ë„ˆë¬´ ì§€ë£¨í•˜ê³  ì¬ë¯¸ì—†ì—ˆìŠµë‹ˆë‹¤",
    "ë°°ìš°ë“¤ì˜ ì—°ê¸°ê°€ í›Œë¥­í–ˆì–´ìš” ì¶”ì²œí•©ë‹ˆë‹¤",
    "ëˆì´ ì•„ê¹Œìš´ ì˜í™”ì˜€ì–´ìš” ë³„ë¡œì…ë‹ˆë‹¤"
]

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì˜ ë²¡í„°í™” ë°©ë²• í™•ì¸
if 'TF-IDF' in best_model_name:
    vectorizer = tfidf_vectorizer
else:
    vectorizer = bow_vectorizer

# ìƒˆë¡œìš´ ë¦¬ë·° ë²¡í„°í™”
new_reviews_vectorized = vectorizer.transform(new_reviews)

# ì˜ˆì¸¡
predictions = best_model.predict(new_reviews_vectorized)
probabilities = best_model.predict_proba(new_reviews_vectorized)

print("ğŸ”® ìƒˆë¡œìš´ ë¦¬ë·° ê°ì„± ì˜ˆì¸¡:")
for i, (review, pred, prob) in enumerate(zip(new_reviews, predictions, probabilities)):
    sentiment = 'ê¸ì •' if pred == 1 else 'ë¶€ì •'
    confidence = prob.max()
    
    print(f"\në¦¬ë·° {i+1}: {review}")
    print(f"  ì˜ˆì¸¡ ê°ì„±: {sentiment}")
    print(f"  í™•ì‹ ë„: {confidence:.3f}")
    print(f"  í™•ë¥  ë¶„í¬: ë¶€ì • {prob[0]:.3f}, ê¸ì • {prob[1]:.3f}")

# 8. ëª¨ë¸ í•´ì„ (íŠ¹ì„± ì¤‘ìš”ë„)
print(f"\n8. ëª¨ë¸ í•´ì„ - {best_model_name}")
print("-" * 30)

if hasattr(best_model, 'coef_'):
    # ë¡œì§€ìŠ¤í‹± íšŒê·€ì˜ ê²½ìš° ê³„ìˆ˜ í™•ì¸
    feature_names = vectorizer.get_feature_names_out()
    coefficients = best_model.coef_[0]
    
    # ê¸ì •ì  ë‹¨ì–´ë“¤ (ê³„ìˆ˜ê°€ ë†’ì€ ë‹¨ì–´)
    positive_indices = coefficients.argsort()[-10:][::-1]
    print("ğŸ˜Š ê¸ì • ê°ì„±ì— ê¸°ì—¬í•˜ëŠ” ë‹¨ì–´ë“¤:")
    for idx in positive_indices:
        print(f"  {feature_names[idx]}: {coefficients[idx]:.3f}")
    
    print()
    
    # ë¶€ì •ì  ë‹¨ì–´ë“¤ (ê³„ìˆ˜ê°€ ë‚®ì€ ë‹¨ì–´)
    negative_indices = coefficients.argsort()[:10]
    print("ğŸ˜ ë¶€ì • ê°ì„±ì— ê¸°ì—¬í•˜ëŠ” ë‹¨ì–´ë“¤:")
    for idx in negative_indices:
        print(f"  {feature_names[idx]}: {coefficients[idx]:.3f}")

print("\n" + "="*50)
print("9. í…ìŠ¤íŠ¸ ë¶„ì„ ì„±ëŠ¥ ê°œì„  ë°©ë²•")
print("="*50)
print("ğŸ“ ë°ì´í„° ì „ì²˜ë¦¬:")
print("  - ë¶ˆìš©ì–´ ì œê±° (ì¡°ì‚¬, ì–´ë¯¸ ë“±)")
print("  - í˜•íƒœì†Œ ë¶„ì„ (ëª…ì‚¬, í˜•ìš©ì‚¬ ì¶”ì¶œ)")
print("  - ë§ì¶¤ë²• êµì •")
print("  - ë™ì˜ì–´ ì²˜ë¦¬")
print()
print("ğŸ”§ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§:")
print("  - N-gram í™œìš© (2-gram, 3-gram)")
print("  - ë‹¨ì–´ ì„ë² ë”© (Word2Vec, FastText)")
print("  - ê°ì • ì‚¬ì „ í™œìš©")
print()
print("ğŸ¤– ëª¨ë¸ ê°œì„ :")
print("  - ë”¥ëŸ¬ë‹ ëª¨ë¸ (LSTM, BERT)")
print("  - ì•™ìƒë¸” ê¸°ë²•")
print("  - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")

print("\n=== í…ìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬ ê¸°ì´ˆ í•™ìŠµ ì™„ë£Œ! ===")
print("ğŸ‰ ê°ì„± ë¶„ì„ ëª¨ë¸ì„ ì„±ê³µì ìœ¼ë¡œ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤!")
print("ğŸ’¡ ë‹¤ìŒ ì£¼ì°¨ì—ì„œëŠ” ì‹¤ì œ AI ì‘ìš© í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤.")
