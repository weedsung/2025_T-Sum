"""
Week 06: ë¨¸ì‹ ëŸ¬ë‹ ì‹¬í™” ì‹¤ìŠµ - ë¶„ë¥˜ ëª¨ë¸
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris, make_classification
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import StandardScaler

print("=== ë¨¸ì‹ ëŸ¬ë‹ ë¶„ë¥˜ ëª¨ë¸ ì‹¬í™” ì‹¤ìŠµ ===\n")

# 1. ë¶„ë¥˜ ë¬¸ì œ ì†Œê°œ
print("1. ë¶„ë¥˜(Classification)ë€?")
print("-" * 30)
print("ğŸ¯ ë¶„ë¥˜: ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ì •ì˜ëœ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œ")
print("ğŸ“ ì˜ˆì‹œ: ìŠ¤íŒ¸ë©”ì¼ ë¶„ë¥˜, ì´ë¯¸ì§€ ì¸ì‹, ì§ˆë³‘ ì§„ë‹¨, ê³ ê° ë¶„ë¥˜")
print("ğŸ”¢ ì´ì§„ ë¶„ë¥˜: Yes/No, ìŠ¤íŒ¸/ì •ìƒ (2ê°œ í´ë˜ìŠ¤)")
print("ğŸŒˆ ë‹¤ì¤‘ ë¶„ë¥˜: ê½ƒì˜ ì¢…ë¥˜, ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ (3ê°œ ì´ìƒ í´ë˜ìŠ¤)")
print()

# 2. Iris ë°ì´í„°ì…‹ ë¡œë“œ
print("2. Iris ê½ƒ ë¶„ë¥˜ ë°ì´í„°ì…‹")
print("-" * 25)

# Iris ë°ì´í„° ë¡œë“œ
iris = load_iris()
X = iris.data  # íŠ¹ì„±: ê½ƒì/ê½ƒë°›ì¹¨ ê¸¸ì´ì™€ ë„ˆë¹„
y = iris.target  # íƒ€ê²Ÿ: ê½ƒì˜ ì¢…ë¥˜ (0: setosa, 1: versicolor, 2: virginica)

# DataFrame ìƒì„±
feature_names = iris.feature_names
target_names = iris.target_names

df = pd.DataFrame(X, columns=feature_names)
df['species'] = pd.Categorical.from_codes(y, target_names)

print(f"ğŸ“Š ë°ì´í„° í¬ê¸°: {df.shape}")
print(f"ğŸŒ¸ ê½ƒì˜ ì¢…ë¥˜: {list(target_names)}")
print(f"ğŸ“ íŠ¹ì„±: {list(feature_names)}")
print("\në°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
print(df.head())
print("\ní´ë˜ìŠ¤ë³„ ë°ì´í„° ê°œìˆ˜:")
print(df['species'].value_counts())
print()

# 3. ë°ì´í„° íƒìƒ‰ ë° ì‹œê°í™”
print("3. ë°ì´í„° íƒìƒ‰ ë° ì‹œê°í™”")
print("-" * 25)

plt.figure(figsize=(15, 10))

# íŠ¹ì„±ë³„ ë¶„í¬ (ì¢…ë¥˜ë³„)
for i, feature in enumerate(feature_names):
    plt.subplot(2, 3, i+1)
    for species in target_names:
        data = df[df['species'] == species][feature]
        plt.hist(data, alpha=0.7, label=species, bins=15)
    plt.xlabel(feature)
    plt.ylabel('ë¹ˆë„')
    plt.title(f'{feature} ë¶„í¬')
    plt.legend()

# ì‚°ì ë„ ë§¤íŠ¸ë¦­ìŠ¤
plt.subplot(2, 3, 5)
colors = ['red', 'green', 'blue']
for i, species in enumerate(target_names):
    species_data = df[df['species'] == species]
    plt.scatter(species_data['sepal length (cm)'], 
               species_data['petal length (cm)'], 
               c=colors[i], label=species, alpha=0.7)
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Petal Length (cm)')
plt.title('Sepal vs Petal Length')
plt.legend()

# ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
plt.subplot(2, 3, 6)
correlation = df.select_dtypes(include=[np.number]).corr()
sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0, fmt='.2f')
plt.title('íŠ¹ì„±ê°„ ìƒê´€ê´€ê³„')

plt.tight_layout()
plt.show()

# 4. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„í• 
print("4. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„í• ")
print("-" * 25)

# í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

print(f"í›ˆë ¨ ë°ì´í„°: {X_train.shape[0]}ê°œ")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape[0]}ê°œ")

# ë°ì´í„° ì •ê·œí™”
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("âœ… ë°ì´í„° ì •ê·œí™” ì™„ë£Œ")
print()

# 5. ë‹¤ì–‘í•œ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ë° ë¹„êµ
print("5. ë‹¤ì–‘í•œ ë¶„ë¥˜ ëª¨ë¸ ë¹„êµ")
print("-" * 25)

# ëª¨ë¸ë“¤ ì •ì˜
models = {
    'Logistic Regression': LogisticRegression(random_state=42),
    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=3),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)
}

results = {}

print("ğŸ¤– ëª¨ë¸ í•™ìŠµ ë° í‰ê°€:")
print("-" * 40)

for name, model in models.items():
    # ëª¨ë¸ í•™ìŠµ
    model.fit(X_train_scaled, y_train)
    
    # ì˜ˆì¸¡
    y_pred = model.predict(X_test_scaled)
    
    # ì •í™•ë„ ê³„ì‚°
    accuracy = accuracy_score(y_test, y_pred)
    
    # êµì°¨ê²€ì¦ ì ìˆ˜
    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)
    
    results[name] = {
        'accuracy': accuracy,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'predictions': y_pred
    }
    
    print(f"ğŸ“Š {name}:")
    print(f"  í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy:.3f}")
    print(f"  êµì°¨ê²€ì¦ í‰ê· : {cv_scores.mean():.3f} (Â±{cv_scores.std():.3f})")
    print()

# 6. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìƒì„¸ ë¶„ì„
print("6. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìƒì„¸ ë¶„ì„")
print("-" * 30)

# ê°€ì¥ ì¢‹ì€ ëª¨ë¸ ì°¾ê¸°
best_model_name = max(results, key=lambda x: results[x]['accuracy'])
best_model = models[best_model_name]
best_predictions = results[best_model_name]['predictions']

print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}")
print(f"ì •í™•ë„: {results[best_model_name]['accuracy']:.3f}")
print()

# ë¶„ë¥˜ ë¦¬í¬íŠ¸
print("ğŸ“‹ ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
print(classification_report(y_test, best_predictions, target_names=target_names))

# í˜¼ë™ í–‰ë ¬
print("ğŸ” í˜¼ë™ í–‰ë ¬ (Confusion Matrix):")
cm = confusion_matrix(y_test, best_predictions)
print(cm)

# í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=target_names, yticklabels=target_names)
plt.title(f'{best_model_name}\ní˜¼ë™ í–‰ë ¬')
plt.xlabel('ì˜ˆì¸¡ í´ë˜ìŠ¤')
plt.ylabel('ì‹¤ì œ í´ë˜ìŠ¤')

# ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ
plt.subplot(1, 2, 2)
model_names = list(results.keys())
accuracies = [results[name]['accuracy'] for name in model_names]
cv_means = [results[name]['cv_mean'] for name in model_names]

x = np.arange(len(model_names))
width = 0.35

plt.bar(x - width/2, accuracies, width, label='í…ŒìŠ¤íŠ¸ ì •í™•ë„', alpha=0.8)
plt.bar(x + width/2, cv_means, width, label='êµì°¨ê²€ì¦ í‰ê· ', alpha=0.8)

plt.xlabel('ëª¨ë¸')
plt.ylabel('ì •í™•ë„')
plt.title('ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ')
plt.xticks(x, [name.split()[0] for name in model_names], rotation=45)
plt.legend()
plt.ylim(0.8, 1.0)

# ì •í™•ë„ ê°’ í‘œì‹œ
for i, (acc, cv) in enumerate(zip(accuracies, cv_means)):
    plt.text(i - width/2, acc + 0.01, f'{acc:.3f}', ha='center', va='bottom')
    plt.text(i + width/2, cv + 0.01, f'{cv:.3f}', ha='center', va='bottom')

plt.tight_layout()
plt.show()

# 7. íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ (Random Forest)
print("7. íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„")
print("-" * 20)

if 'Random Forest' in models:
    rf_model = models['Random Forest']
    feature_importance = rf_model.feature_importances_
    
    # íŠ¹ì„± ì¤‘ìš”ë„ ë°ì´í„°í”„ë ˆì„
    importance_df = pd.DataFrame({
        'íŠ¹ì„±': feature_names,
        'ì¤‘ìš”ë„': feature_importance
    }).sort_values('ì¤‘ìš”ë„', ascending=False)
    
    print("ğŸ” íŠ¹ì„± ì¤‘ìš”ë„ ìˆœìœ„:")
    for i, row in importance_df.iterrows():
        print(f"  {row['íŠ¹ì„±']}: {row['ì¤‘ìš”ë„']:.3f}")
    
    # íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”
    plt.figure(figsize=(10, 6))
    sns.barplot(data=importance_df, x='ì¤‘ìš”ë„', y='íŠ¹ì„±', palette='viridis')
    plt.title('Random Forest íŠ¹ì„± ì¤‘ìš”ë„')
    plt.xlabel('ì¤‘ìš”ë„')
    
    # ê°’ í‘œì‹œ
    for i, v in enumerate(importance_df['ì¤‘ìš”ë„']):
        plt.text(v + 0.01, i, f'{v:.3f}', va='center')
    
    plt.tight_layout()
    plt.show()

print()

# 8. ìƒˆë¡œìš´ ë°ì´í„° ì˜ˆì¸¡ ì‹¤ìŠµ
print("8. ìƒˆë¡œìš´ ë°ì´í„° ì˜ˆì¸¡ ì‹¤ìŠµ")
print("-" * 25)

# ìƒˆë¡œìš´ ê½ƒ ë°ì´í„° (ì„ì˜ ìƒì„±)
new_flowers = np.array([
    [5.1, 3.5, 1.4, 0.2],  # setosaì™€ ìœ ì‚¬
    [6.2, 2.8, 4.8, 1.8],  # versicolorì™€ ìœ ì‚¬
    [7.3, 2.9, 6.3, 1.8]   # virginicaì™€ ìœ ì‚¬
])

# ì •ê·œí™”
new_flowers_scaled = scaler.transform(new_flowers)

# ì˜ˆì¸¡
predictions = best_model.predict(new_flowers_scaled)
probabilities = best_model.predict_proba(new_flowers_scaled)

print("ğŸ”® ìƒˆë¡œìš´ ê½ƒ ë¶„ë¥˜ ì˜ˆì¸¡:")
for i, (flower, pred, prob) in enumerate(zip(new_flowers, predictions, probabilities)):
    predicted_species = target_names[pred]
    confidence = prob.max()
    
    print(f"\nê½ƒ {i+1}: {flower}")
    print(f"  ì˜ˆì¸¡ ì¢…ë¥˜: {predicted_species}")
    print(f"  í™•ì‹ ë„: {confidence:.2f}")
    print(f"  ê° í´ë˜ìŠ¤ í™•ë¥ :")
    for j, (species, p) in enumerate(zip(target_names, prob)):
        print(f"    {species}: {p:.3f}")

# 9. ëª¨ë¸ ì„±ëŠ¥ ê°œì„  íŒ
print("\n" + "="*50)
print("9. ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥ ê°œì„  íŒ")
print("="*50)
print("ğŸ’¡ ë°ì´í„° ê´€ë ¨:")
print("  - ë” ë§ì€ ë°ì´í„° ìˆ˜ì§‘")
print("  - ë°ì´í„° í’ˆì§ˆ ê°œì„  (ì´ìƒì¹˜, ê²°ì¸¡ì¹˜ ì²˜ë¦¬)")
print("  - íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ (ìƒˆë¡œìš´ íŠ¹ì„± ìƒì„±)")
print()
print("ğŸ”§ ëª¨ë¸ ê´€ë ¨:")
print("  - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
print("  - ì•™ìƒë¸” ê¸°ë²• í™œìš©")
print("  - êµì°¨ê²€ì¦ìœ¼ë¡œ ì•ˆì •ì„± í™•ì¸")
print()
print("âš–ï¸ í‰ê°€ ê´€ë ¨:")
print("  - ì ì ˆí•œ í‰ê°€ ì§€í‘œ ì„ íƒ")
print("  - í´ë˜ìŠ¤ ë¶ˆê· í˜• ê³ ë ¤")
print("  - ë¹„ì¦ˆë‹ˆìŠ¤ ëª©í‘œì™€ ì—°ê²°")

print("\n=== ë¶„ë¥˜ ëª¨ë¸ ì‹¬í™” í•™ìŠµ ì™„ë£Œ! ===")
print("ğŸ‰ ë‹¤ì–‘í•œ ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ì„ ë¹„êµí•´ë³´ì•˜ìŠµë‹ˆë‹¤!")
print("ğŸ’¡ ë‹¤ìŒ ì£¼ì°¨ì—ì„œëŠ” í…ìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬ë¥¼ ë°°ì›Œë³´ê² ìŠµë‹ˆë‹¤.")
